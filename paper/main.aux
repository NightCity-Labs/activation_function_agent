\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{glorot2011relu,nair2010relu}
\citation{ramachandran2017swish}
\citation{hendrycks2016gelu}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{2}{Introduction}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Learned activation function from optimizing correlation objective on 20-class Gaussian mixture with moderate separation ($\sigma =1.0$). The emergent shape is Swish-like ($f(x) \approx x \cdot \sigma (2.5x)$), derived from first principles without hand-design.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:main_result}{{1}{2}{Learned activation function from optimizing correlation objective on 20-class Gaussian mixture with moderate separation ($\sigma =1.0$). The emergent shape is Swish-like ($f(x) \approx x \cdot \sigma (2.5x)$), derived from first principles without hand-design}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theoretical Framework}{2}{section.2}\protected@file@percent }
\newlabel{sec:theory}{{2}{2}{Theoretical Framework}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Correlation Objective}{2}{subsection.2.1}\protected@file@percent }
\citation{krizhevsky2009cifar}
\citation{simonyan2014vgg}
\newlabel{eq:objective}{{2}{3}{Correlation Objective}{equation.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gaussian Mixture Setting}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Analytical Structure}{3}{subsection.2.3}\protected@file@percent }
\newlabel{eq:swish}{{6}{3}{Analytical Structure}{equation.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{3}{section.3}\protected@file@percent }
\newlabel{sec:methods}{{3}{3}{Methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Unit-Level Experiments}{3}{subsection.3.1}\protected@file@percent }
\citation{he2016resnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Neural Network Experiments}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{4}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{4}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Unit-Level Results}{4}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Neural Network Results}{4}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Unit-level results from Gaussian mixture experiments.} (a) Two-class case: ReLU-like shape emerges as optimal. (b) 20-class mid-$\sigma $: Swish-like shape with fitted curve (red) showing excellent agreement ($\beta \approx 2.5$, R$^2 > 0.99$). (c) Variance sweep: Small $\sigma $ produces sharper activation, large $\sigma $ produces smoother, broader curve.}}{5}{figure.2}\protected@file@percent }
\newlabel{fig:unit_results}{{2}{5}{\textbf {Unit-level results from Gaussian mixture experiments.} (a) Two-class case: ReLU-like shape emerges as optimal. (b) 20-class mid-$\sigma $: Swish-like shape with fitted curve (red) showing excellent agreement ($\beta \approx 2.5$, R$^2 > 0.99$). (c) Variance sweep: Small $\sigma $ produces sharper activation, large $\sigma $ produces smoother, broader curve}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Evolution of learned activations during neural network training.} Activation functions start near-linear (ReLU initialization) and converge to Swish-like shapes. Different layers show different convergence rates and final $\beta $ values.}}{5}{figure.3}\protected@file@percent }
\newlabel{fig:nn_evolution}{{3}{5}{\textbf {Evolution of learned activations during neural network training.} Activation functions start near-linear (ReLU initialization) and converge to Swish-like shapes. Different layers show different convergence rates and final $\beta $ values}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Layer-wise activation comparison in ResNet-10.} Early layers (left) learn broader, smoother activations; late layers (right) learn sharper, more ReLU-like activations. This reflects the changing pre-activation statistics across network depth.}}{5}{figure.4}\protected@file@percent }
\newlabel{fig:layer_comparison}{{4}{5}{\textbf {Layer-wise activation comparison in ResNet-10.} Early layers (left) learn broader, smoother activations; late layers (right) learn sharper, more ReLU-like activations. This reflects the changing pre-activation statistics across network depth}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Swish Fitting Analysis}{5}{subsection.4.3}\protected@file@percent }
\citation{ramachandran2017swish}
\citation{agostinelli2014learning}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{6}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{6}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Why Swish Emerges}{6}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Connection to Backpropagation}{6}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Relationship to Prior Work}{6}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Limitations}{6}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Future Directions}{6}{subsection.5.5}\protected@file@percent }
\bibdata{references}
\bibcite{agostinelli2014learning}{{1}{}{{}}{{}}}
\bibcite{glorot2011relu}{{2}{}{{}}{{}}}
\bibcite{he2016resnet}{{3}{}{{}}{{}}}
\bibcite{hendrycks2016gelu}{{4}{}{{}}{{}}}
\bibcite{krizhevsky2009cifar}{{5}{}{{}}{{}}}
\bibcite{nair2010relu}{{6}{}{{}}{{}}}
\bibcite{ramachandran2017swish}{{7}{}{{}}{{}}}
\bibcite{simonyan2014vgg}{{8}{}{{}}{{}}}
\bibstyle{plain}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{7}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{7}{Conclusion}{section.6}{}}
\gdef \@abspage@last{7}
